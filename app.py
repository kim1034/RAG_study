import streamlit as st

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ë‚˜ì˜ AI ë¬¸ì„œ ë¹„ì„œ", page_icon="ğŸ¤–")
st.title("ğŸ“„ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš” (RAG)")

# 2. ë¡œì§ (ë°ì´í„° ë¡œë“œ ë° ì²´ì¸ ìƒì„±)
@st.cache_resource # ì•±ì´ ìƒˆë¡œê³ ì¹¨ë˜ì–´ë„ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê²Œ í•´ì£¼ëŠ” ê³ ë§ˆìš´ ê¸°ëŠ¥
def setup_rag():
    loader = PyPDFLoader("study_data.pdf")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)
    
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-miniLm-l6-v2")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest")
    
    prompt = ChatPromptTemplate.from_template("""
    ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
    {context}
    
    ì§ˆë¬¸: {input}
    """)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
        
    rag_chain = (
        {"context": vectorstore.as_retriever() | format_docs, "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# 3. í™”ë©´ êµ¬ì„±
chain = setup_rag()

if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì±„íŒ… ì…ë ¥ì°½
if prompt_input := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì ì–´ì£¼ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    with st.chat_message("user"):
        st.markdown(prompt_input)

    with st.chat_message("assistant"):
        response = chain.invoke(prompt_input)
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})